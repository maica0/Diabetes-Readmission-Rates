{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML GROUP ASSIGNMENT:\n",
    "### Predicting readmissions by leveraging \"diabetic_data.csv\"\n",
    "MBD Section 2, Group 6\n",
    "\n",
    "Group members & notebook credit:\n",
    "- Ignacio Ferro\n",
    "- Jose Carranque\n",
    "- Maica Muñoz\n",
    "- Maria Jose Perez\n",
    "- Mohammed Alotaibi\n",
    "- Rodrigo Reyes Sanchez\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0 - Loading key libraries & dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101766, 50)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#Loading Dataset\n",
    "df = pd.read_csv(\"diabetic_data.csv\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "race 2273\n",
      "gender 0\n",
      "age 0\n",
      "weight 98569\n",
      "payer_code 40256\n",
      "medical_specialty 49949\n",
      "diag_1 21\n",
      "diag_2 358\n",
      "diag_3 1423\n",
      "max_glu_serum 0\n",
      "A1Cresult 0\n",
      "metformin 0\n",
      "repaglinide 0\n",
      "nateglinide 0\n",
      "chlorpropamide 0\n",
      "glimepiride 0\n",
      "acetohexamide 0\n",
      "glipizide 0\n",
      "glyburide 0\n",
      "tolbutamide 0\n",
      "pioglitazone 0\n",
      "rosiglitazone 0\n",
      "acarbose 0\n",
      "miglitol 0\n",
      "troglitazone 0\n",
      "tolazamide 0\n",
      "examide 0\n",
      "citoglipton 0\n",
      "insulin 0\n",
      "glyburide-metformin 0\n",
      "glipizide-metformin 0\n",
      "glimepiride-pioglitazone 0\n",
      "metformin-rosiglitazone 0\n",
      "metformin-pioglitazone 0\n",
      "change 0\n",
      "diabetesMed 0\n",
      "readmitted 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "readmitted\n",
       "NO     54864\n",
       ">30    35545\n",
       "<30    11357\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify number of null values for each feature\n",
    "df.isna().sum()\n",
    "\n",
    "#Identify number of missing values (represented as '?' sign) for each feature\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == object:\n",
    "         print(col,df[col][df[col] == '?'].count())\n",
    "\n",
    "# Analyze the distribution of values for Numerical features (identify outliers)\n",
    "df['time_in_hospital'].describe()\n",
    "df['num_lab_procedures'].describe()\n",
    "df['num_medications'].describe()\n",
    "df['number_outpatient'].describe()\n",
    "df['number_emergency'].describe()\n",
    "df['number_inpatient'].describe()\n",
    "df['number_diagnoses'].describe()\n",
    "\n",
    "# Analyze the distribution of values for Categorical features (identify outliers)\n",
    "df['race'].value_counts()\n",
    "df['gender'].value_counts()\n",
    "df['age'].value_counts()\n",
    "df['weight'].value_counts()\n",
    "df['num_procedures'].value_counts()\n",
    "df['admission_type_id'].value_counts()\n",
    "df['discharge_disposition_id'].value_counts()\n",
    "df['admission_source_id'].value_counts()\n",
    "df['payer_code'].value_counts()\n",
    "df['medical_specialty'].value_counts()\n",
    "df['diag_1'].value_counts()\n",
    "df['diag_2'].value_counts()\n",
    "df['diag_3'].value_counts()\n",
    "df['max_glu_serum'].value_counts()\n",
    "df['A1Cresult'].value_counts()\n",
    "df['metformin'].value_counts()\n",
    "df['repaglinide'].value_counts()\n",
    "df['nateglinide'].value_counts()\n",
    "df['chlorpropamide'].value_counts()\n",
    "df['glimepiride'].value_counts()\n",
    "df['acetohexamide'].value_counts()\n",
    "df['glipizide'].value_counts()\n",
    "df['glyburide'].value_counts()\n",
    "df['tolbutamide'].value_counts()\n",
    "df['pioglitazone'].value_counts()\n",
    "df['rosiglitazone'].value_counts()\n",
    "df['acarbose'].value_counts()\n",
    "df['miglitol'].value_counts()\n",
    "df['troglitazone'].value_counts()\n",
    "df['tolazamide'].value_counts()\n",
    "df['examide'].value_counts()\n",
    "df['citoglipton'].value_counts()\n",
    "df['insulin'].value_counts()\n",
    "df['glyburide-metformin'].value_counts()\n",
    "df['glipizide-metformin'].value_counts()\n",
    "df['glimepiride-pioglitazone'].value_counts()\n",
    "df['metformin-rosiglitazone'].value_counts()\n",
    "df['metformin-pioglitazone'].value_counts()\n",
    "df['change'].value_counts()\n",
    "df['diabetesMed'].value_counts()\n",
    "df['num_procedures'].value_counts()\n",
    "df['readmitted'].value_counts()\n",
    "\n",
    "\n",
    "# ---- CODE TO PLOT HISTOGRAMS ----\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# plt.hist(df['num_medications'], color='skyblue', edgecolor='black')\n",
    "# plt.xlabel('Number of Medications')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.title('Distribution of Number of Medications')\n",
    "# plt.grid(True)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Data Cleaning & Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ CREATE NEW COLUMNS ------\n",
    "\n",
    "# Create a new column called \"encounter_number\".\n",
    "# It counts the number of encounters that each unique \"patient_nbr\" did until that moment (ordinal encoding)\n",
    "df['encounter_number'] = df.groupby('patient_nbr').cumcount() + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplification of diag_1, diag_2 and diag_3\n",
    "Source & credit: https://www.kaggle.com/code/iabhishekofficial/prediction-on-hospital-readmission?scriptVersionId=14883095&cellId=35\n",
    "\n",
    "Categorization of diagnoses: The dataset contained up to three diagnoses for a given patient (primary, secondary and additional). However, each of these had 700–900 unique ICD codes and it is extremely difficult to include them in the model and interpret meaningfully. Therefore, we collapsed these diagnosis codes into 9 disease categories in an almost similar fashion to that done in the original publication using this dataset. These 9 categories include Circulatory, Respiratory, Digestive, Diabetes, Injury, Musculoskeletal, Genitourinary, Neoplasms, and Others.\n",
    "\n",
    "Although we did this for primary, secondary and additional diagnoses, we eventually decided to use only the primary diagnosis in our model. Doing this in Python was slightly cumbersome because, well, we are mapping the disease codes to certain category names.\n",
    "\n",
    "The code below should demonstrate this easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ SIMPLIFICATION OF diag_1, diag_2 and diag_3 ------\n",
    "\n",
    "# Creating additional columns for diagnosis\n",
    "df['level1_diag1'] = df['diag_1']\n",
    "df['level2_diag1'] = df['diag_1']\n",
    "df['level1_diag2'] = df['diag_2']\n",
    "df['level2_diag2'] = df['diag_2']\n",
    "df['level1_diag3'] = df['diag_3']\n",
    "df['level2_diag3'] = df['diag_3']\n",
    "\n",
    "df.loc[df['diag_1'].str.contains('V'), ['level1_diag1', 'level2_diag1']] = 0\n",
    "df.loc[df['diag_1'].str.contains('E'), ['level1_diag1', 'level2_diag1']] = 0\n",
    "df.loc[df['diag_2'].str.contains('V'), ['level1_diag2', 'level2_diag2']] = 0\n",
    "df.loc[df['diag_2'].str.contains('E'), ['level1_diag2', 'level2_diag2']] = 0\n",
    "df.loc[df['diag_3'].str.contains('V'), ['level1_diag3', 'level2_diag3']] = 0\n",
    "df.loc[df['diag_3'].str.contains('E'), ['level1_diag3', 'level2_diag3']] = 0\n",
    "df['level1_diag1'] = df['level1_diag1'].replace('?', -1)\n",
    "df['level2_diag1'] = df['level2_diag1'].replace('?', -1)\n",
    "df['level1_diag2'] = df['level1_diag2'].replace('?', -1)\n",
    "df['level2_diag2'] = df['level2_diag2'].replace('?', -1)\n",
    "df['level1_diag3'] = df['level1_diag3'].replace('?', -1)\n",
    "df['level2_diag3'] = df['level2_diag3'].replace('?', -1)\n",
    "\n",
    "df['level1_diag1'] = df['level1_diag1'].astype(float)\n",
    "df['level2_diag1'] = df['level2_diag1'].astype(float)\n",
    "df['level1_diag2'] = df['level1_diag2'].astype(float)\n",
    "df['level2_diag2'] = df['level2_diag2'].astype(float)\n",
    "df['level1_diag3'] = df['level1_diag3'].astype(float)\n",
    "df['level2_diag3'] = df['level2_diag3'].astype(float)\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if (row['level1_diag1'] >= 390 and row['level1_diag1'] < 460) or (np.floor(row['level1_diag1']) == 785):\n",
    "        df.loc[index, 'level1_diag1'] = 1\n",
    "    elif (row['level1_diag1'] >= 460 and row['level1_diag1'] < 520) or (np.floor(row['level1_diag1']) == 786):\n",
    "        df.loc[index, 'level1_diag1'] = 2\n",
    "    elif (row['level1_diag1'] >= 520 and row['level1_diag1'] < 580) or (np.floor(row['level1_diag1']) == 787):\n",
    "        df.loc[index, 'level1_diag1'] = 3\n",
    "    elif (np.floor(row['level1_diag1']) == 250):\n",
    "        df.loc[index, 'level1_diag1'] = 4\n",
    "    elif (row['level1_diag1'] >= 800 and row['level1_diag1'] < 1000):\n",
    "        df.loc[index, 'level1_diag1'] = 5\n",
    "    elif (row['level1_diag1'] >= 710 and row['level1_diag1'] < 740):\n",
    "        df.loc[index, 'level1_diag1'] = 6\n",
    "    elif (row['level1_diag1'] >= 580 and row['level1_diag1'] < 630) or (np.floor(row['level1_diag1']) == 788):\n",
    "        df.loc[index, 'level1_diag1'] = 7\n",
    "    elif (row['level1_diag1'] >= 140 and row['level1_diag1'] < 240):\n",
    "        df.loc[index, 'level1_diag1'] = 8\n",
    "    else:\n",
    "        df.loc[index, 'level1_diag1'] = 0\n",
    "        \n",
    "    if (row['level1_diag2'] >= 390 and row['level1_diag2'] < 460) or (np.floor(row['level1_diag2']) == 785):\n",
    "        df.loc[index, 'level1_diag2'] = 1\n",
    "    elif (row['level1_diag2'] >= 460 and row['level1_diag2'] < 520) or (np.floor(row['level1_diag2']) == 786):\n",
    "        df.loc[index, 'level1_diag2'] = 2\n",
    "    elif (row['level1_diag2'] >= 520 and row['level1_diag2'] < 580) or (np.floor(row['level1_diag2']) == 787):\n",
    "        df.loc[index, 'level1_diag2'] = 3\n",
    "    elif (np.floor(row['level1_diag2']) == 250):\n",
    "        df.loc[index, 'level1_diag2'] = 4\n",
    "    elif (row['level1_diag2'] >= 800 and row['level1_diag2'] < 1000):\n",
    "        df.loc[index, 'level1_diag2'] = 5\n",
    "    elif (row['level1_diag2'] >= 710 and row['level1_diag2'] < 740):\n",
    "        df.loc[index, 'level1_diag2'] = 6\n",
    "    elif (row['level1_diag2'] >= 580 and row['level1_diag2'] < 630) or (np.floor(row['level1_diag2']) == 788):\n",
    "        df.loc[index, 'level1_diag2'] = 7\n",
    "    elif (row['level1_diag2'] >= 140 and row['level1_diag2'] < 240):\n",
    "        df.loc[index, 'level1_diag2'] = 8\n",
    "    else:\n",
    "        df.loc[index, 'level1_diag2'] = 0\n",
    "    \n",
    "    if (row['level1_diag3'] >= 390 and row['level1_diag3'] < 460) or (np.floor(row['level1_diag3']) == 785):\n",
    "        df.loc[index, 'level1_diag3'] = 1\n",
    "    elif (row['level1_diag3'] >= 460 and row['level1_diag3'] < 520) or (np.floor(row['level1_diag3']) == 786):\n",
    "        df.loc[index, 'level1_diag3'] = 2\n",
    "    elif (row['level1_diag3'] >= 520 and row['level1_diag3'] < 580) or (np.floor(row['level1_diag3']) == 787):\n",
    "        df.loc[index, 'level1_diag3'] = 3\n",
    "    elif (np.floor(row['level1_diag3']) == 250):\n",
    "        df.loc[index, 'level1_diag3'] = 4\n",
    "    elif (row['level1_diag3'] >= 800 and row['level1_diag3'] < 1000):\n",
    "        df.loc[index, 'level1_diag3'] = 5\n",
    "    elif (row['level1_diag3'] >= 710 and row['level1_diag3'] < 740):\n",
    "        df.loc[index, 'level1_diag3'] = 6\n",
    "    elif (row['level1_diag3'] >= 580 and row['level1_diag3'] < 630) or (np.floor(row['level1_diag3']) == 788):\n",
    "        df.loc[index, 'level1_diag3'] = 7\n",
    "    elif (row['level1_diag3'] >= 140 and row['level1_diag3'] < 240):\n",
    "        df.loc[index, 'level1_diag3'] = 8\n",
    "    else:\n",
    "        df.loc[index, 'level1_diag3'] = 0\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if (row['level2_diag1'] >= 390 and row['level2_diag1'] < 399):\n",
    "        df.loc[index, 'level2_diag1'] = 1\n",
    "    elif (row['level2_diag1'] >= 401 and row['level2_diag1'] < 415):\n",
    "        df.loc[index, 'level2_diag1'] = 2\n",
    "    elif (row['level2_diag1'] >= 415 and row['level2_diag1'] < 460):\n",
    "        df.loc[index, 'level2_diag1'] = 3\n",
    "    elif (np.floor(row['level2_diag1']) == 785):\n",
    "        df.loc[index, 'level2_diag1'] = 4\n",
    "    elif (row['level2_diag1'] >= 460 and row['level2_diag1'] < 489):\n",
    "        df.loc[index, 'level2_diag1'] = 5\n",
    "    elif (row['level2_diag1'] >= 490 and row['level2_diag1'] < 497):\n",
    "        df.loc[index, 'level2_diag1'] = 6\n",
    "    elif (row['level2_diag1'] >= 500 and row['level2_diag1'] < 520):\n",
    "        df.loc[index, 'level2_diag1'] = 7\n",
    "    elif (np.floor(row['level2_diag1']) == 786):\n",
    "        df.loc[index, 'level2_diag1'] = 8\n",
    "    elif (row['level2_diag1'] >= 520 and row['level2_diag1'] < 530):\n",
    "        df.loc[index, 'level2_diag1'] = 9\n",
    "    elif (row['level2_diag1'] >= 530 and row['level2_diag1'] < 544):\n",
    "        df.loc[index, 'level2_diag1'] = 10\n",
    "    elif (row['level2_diag1'] >= 550 and row['level2_diag1'] < 554):\n",
    "        df.loc[index, 'level2_diag1'] = 11\n",
    "    elif (row['level2_diag1'] >= 555 and row['level2_diag1'] < 580):\n",
    "        df.loc[index, 'level2_diag1'] = 12\n",
    "    elif (np.floor(row['level2_diag1']) == 787):\n",
    "        df.loc[index, 'level2_diag1'] = 13\n",
    "    elif (np.floor(row['level2_diag1']) == 250):\n",
    "        df.loc[index, 'level2_diag1'] = 14\n",
    "    elif (row['level2_diag1'] >= 800 and row['level2_diag1'] < 1000):\n",
    "        df.loc[index, 'level2_diag1'] = 15\n",
    "    elif (row['level2_diag1'] >= 710 and row['level2_diag1'] < 740):\n",
    "        df.loc[index, 'level2_diag1'] = 16\n",
    "    elif (row['level2_diag1'] >= 580 and row['level2_diag1'] < 630):\n",
    "        df.loc[index, 'level2_diag1'] = 17\n",
    "    elif (np.floor(row['level2_diag1']) == 788):\n",
    "        df.loc[index, 'level2_diag1'] = 18\n",
    "    elif (row['level2_diag1'] >= 140 and row['level2_diag1'] < 240):\n",
    "        df.loc[index, 'level2_diag1'] = 19\n",
    "    elif row['level2_diag1'] >= 240 and row['level2_diag1'] < 280 and (np.floor(row['level2_diag1']) != 250):\n",
    "        df.loc[index, 'level2_diag1'] = 20\n",
    "    elif (row['level2_diag1'] >= 680 and row['level2_diag1'] < 710) or (np.floor(row['level2_diag1']) == 782):\n",
    "        df.loc[index, 'level2_diag1'] = 21\n",
    "    elif (row['level2_diag1'] >= 290 and row['level2_diag1'] < 320):\n",
    "        df.loc[index, 'level2_diag1'] = 22\n",
    "    else:\n",
    "        df.loc[index, 'level2_diag1'] = 0\n",
    "        \n",
    "    if (row['level2_diag2'] >= 390 and row['level2_diag2'] < 399):\n",
    "        df.loc[index, 'level2_diag2'] = 1\n",
    "    elif (row['level2_diag2'] >= 401 and row['level2_diag2'] < 415):\n",
    "        df.loc[index, 'level2_diag2'] = 2\n",
    "    elif (row['level2_diag2'] >= 415 and row['level2_diag2'] < 460):\n",
    "        df.loc[index, 'level2_diag2'] = 3\n",
    "    elif (np.floor(row['level2_diag2']) == 785):\n",
    "        df.loc[index, 'level2_diag2'] = 4\n",
    "    elif (row['level2_diag2'] >= 460 and row['level2_diag2'] < 489):\n",
    "        df.loc[index, 'level2_diag2'] = 5\n",
    "    elif (row['level2_diag2'] >= 490 and row['level2_diag2'] < 497):\n",
    "        df.loc[index, 'level2_diag2'] = 6\n",
    "    elif (row['level2_diag2'] >= 500 and row['level2_diag2'] < 520):\n",
    "        df.loc[index, 'level2_diag2'] = 7\n",
    "    elif (np.floor(row['level2_diag2']) == 786):\n",
    "        df.loc[index, 'level2_diag2'] = 8\n",
    "    elif (row['level2_diag2'] >= 520 and row['level2_diag2'] < 530):\n",
    "        df.loc[index, 'level2_diag2'] = 9\n",
    "    elif (row['level2_diag2'] >= 530 and row['level2_diag2'] < 544):\n",
    "        df.loc[index, 'level2_diag2'] = 10\n",
    "    elif (row['level2_diag2'] >= 550 and row['level2_diag2'] < 554):\n",
    "        df.loc[index, 'level2_diag2'] = 11\n",
    "    elif (row['level2_diag2'] >= 555 and row['level2_diag2'] < 580):\n",
    "        df.loc[index, 'level2_diag2'] = 12\n",
    "    elif (np.floor(row['level2_diag2']) == 787):\n",
    "        df.loc[index, 'level2_diag2'] = 13\n",
    "    elif (np.floor(row['level2_diag2']) == 250):\n",
    "        df.loc[index, 'level2_diag2'] = 14\n",
    "    elif (row['level2_diag2'] >= 800 and row['level2_diag2'] < 1000):\n",
    "        df.loc[index, 'level2_diag2'] = 15\n",
    "    elif (row['level2_diag2'] >= 710 and row['level2_diag2'] < 740):\n",
    "        df.loc[index, 'level2_diag2'] = 16\n",
    "    elif (row['level2_diag2'] >= 580 and row['level2_diag2'] < 630):\n",
    "        df.loc[index, 'level2_diag2'] = 17\n",
    "    elif (np.floor(row['level2_diag2']) == 788):\n",
    "        df.loc[index, 'level2_diag2'] = 18\n",
    "    elif (row['level2_diag2'] >= 140 and row['level2_diag2'] < 240):\n",
    "        df.loc[index, 'level2_diag2'] = 19\n",
    "    elif row['level2_diag2'] >= 240 and row['level2_diag2'] < 280 and (np.floor(row['level2_diag2']) != 250):\n",
    "        df.loc[index, 'level2_diag2'] = 20\n",
    "    elif (row['level2_diag2'] >= 680 and row['level2_diag2'] < 710) or (np.floor(row['level2_diag2']) == 782):\n",
    "        df.loc[index, 'level2_diag2'] = 21\n",
    "    elif (row['level2_diag2'] >= 290 and row['level2_diag2'] < 320):\n",
    "        df.loc[index, 'level2_diag2'] = 22\n",
    "    else:\n",
    "        df.loc[index, 'level2_diag2'] = 0\n",
    "        \n",
    "        \n",
    "    if (row['level2_diag3'] >= 390 and row['level2_diag3'] < 399):\n",
    "        df.loc[index, 'level2_diag3'] = 1\n",
    "    elif (row['level2_diag3'] >= 401 and row['level2_diag3'] < 415):\n",
    "        df.loc[index, 'level2_diag3'] = 2\n",
    "    elif (row['level2_diag3'] >= 415 and row['level2_diag3'] < 460):\n",
    "        df.loc[index, 'level2_diag3'] = 3\n",
    "    elif (np.floor(row['level2_diag3']) == 785):\n",
    "        df.loc[index, 'level2_diag3'] = 4\n",
    "    elif (row['level2_diag3'] >= 460 and row['level2_diag3'] < 489):\n",
    "        df.loc[index, 'level2_diag3'] = 5\n",
    "    elif (row['level2_diag3'] >= 490 and row['level2_diag3'] < 497):\n",
    "        df.loc[index, 'level2_diag3'] = 6\n",
    "    elif (row['level2_diag3'] >= 500 and row['level2_diag3'] < 520):\n",
    "        df.loc[index, 'level2_diag3'] = 7\n",
    "    elif (np.floor(row['level2_diag3']) == 786):\n",
    "        df.loc[index, 'level2_diag3'] = 8\n",
    "    elif (row['level2_diag3'] >= 520 and row['level2_diag3'] < 530):\n",
    "        df.loc[index, 'level2_diag3'] = 9\n",
    "    elif (row['level2_diag3'] >= 530 and row['level2_diag3'] < 544):\n",
    "        df.loc[index, 'level2_diag3'] = 10\n",
    "    elif (row['level2_diag3'] >= 550 and row['level2_diag3'] < 554):\n",
    "        df.loc[index, 'level2_diag3'] = 11\n",
    "    elif (row['level2_diag3'] >= 555 and row['level2_diag3'] < 580):\n",
    "        df.loc[index, 'level2_diag3'] = 12\n",
    "    elif (np.floor(row['level2_diag3']) == 787):\n",
    "        df.loc[index, 'level2_diag3'] = 13\n",
    "    elif (np.floor(row['level2_diag3']) == 250):\n",
    "        df.loc[index, 'level2_diag3'] = 14\n",
    "    elif (row['level2_diag3'] >= 800 and row['level2_diag3'] < 1000):\n",
    "        df.loc[index, 'level2_diag3'] = 15\n",
    "    elif (row['level2_diag3'] >= 710 and row['level2_diag3'] < 740):\n",
    "        df.loc[index, 'level2_diag3'] = 16\n",
    "    elif (row['level2_diag3'] >= 580 and row['level2_diag3'] < 630):\n",
    "        df.loc[index, 'level2_diag3'] = 17\n",
    "    elif (np.floor(row['level2_diag3']) == 788):\n",
    "        df.loc[index, 'level2_diag3'] = 18\n",
    "    elif (row['level2_diag3'] >= 140 and row['level2_diag3'] < 240):\n",
    "        df.loc[index, 'level2_diag3'] = 19\n",
    "    elif row['level2_diag3'] >= 240 and row['level2_diag3'] < 280 and (np.floor(row['level2_diag3']) != 250):\n",
    "        df.loc[index, 'level2_diag3'] = 20\n",
    "    elif (row['level2_diag3'] >= 680 and row['level2_diag3'] < 710) or (np.floor(row['level2_diag3']) == 782):\n",
    "        df.loc[index, 'level2_diag3'] = 21\n",
    "    elif (row['level2_diag3'] >= 290 and row['level2_diag3'] < 320):\n",
    "        df.loc[index, 'level2_diag3'] = 22\n",
    "    else:\n",
    "        df.loc[index, 'level2_diag3'] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ DROP USELESS COLUMNS ------\n",
    "features_to_drop = ['encounter_id',\n",
    "                    'patient_nbr',\n",
    "                    'weight', \n",
    "                    'payer_code',\n",
    "                    'medical_specialty',\n",
    "                    'diag_1',\n",
    "                    'diag_2',\n",
    "                    'diag_3',\n",
    "                    'repaglinide',\n",
    "                    'nateglinide',\n",
    "                    'chlorpropamide',\n",
    "                    'acetohexamide',\n",
    "                    'tolbutamide',\n",
    "                    'acarbose',\n",
    "                    'miglitol',\n",
    "                    'troglitazone',\n",
    "                    'tolazamide',\n",
    "                    'examide',\n",
    "                    'citoglipton',\n",
    "                    'glyburide-metformin',\n",
    "                    'glipizide-metformin',\n",
    "                    'glimepiride-pioglitazone',\n",
    "                    'metformin-rosiglitazone',\n",
    "                    'metformin-pioglitazone']\n",
    "df = df.drop(features_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ DROP ROWS WITH MISSING VALUES AND/OR OUTLIERS (THAT ARE NOT CRITICAL FOR THE MODEL) ------\n",
    "\n",
    "# Drop rows that have useless values in \"discharge_disposition_id\" column\n",
    "# Values of 11, 13, 14, 19, 20, or 21 are related to death or hospice which mean these patients cannot be readmitted.\n",
    "df = df[df['discharge_disposition_id'] != '11']\n",
    "df = df[df['discharge_disposition_id'] != '13']\n",
    "df = df[df['discharge_disposition_id'] != '14']\n",
    "df = df[df['discharge_disposition_id'] != '19']\n",
    "df = df[df['discharge_disposition_id'] != '20']\n",
    "df = df[df['discharge_disposition_id'] != '21']\n",
    "\n",
    "# Remove rows that have missing values in \"race\" column\n",
    "df = df[df['race'] != '?']\n",
    "\n",
    "# Drop rows that have outlier values in \"gender\" column\n",
    "df = df[df['gender'] != 'Unknown/Invalid']\n",
    "\n",
    "# Drop rows that have outlier values in \"metformin\" column\n",
    "df = df[df['metformin'] != 'Up']\n",
    "df = df[df['metformin'] != 'Down']\n",
    "\n",
    "# Drop rows that have outlier values in \"glimepiride\" column\n",
    "df = df[df['glimepiride'] != 'Up']\n",
    "df = df[df['glimepiride'] != 'Down']\n",
    "\n",
    "# Drop rows that have outlier values in \"glipizide\" column\n",
    "df = df[df['glipizide'] != 'Up']\n",
    "df = df[df['glipizide'] != 'Down']\n",
    "\n",
    "# Drop rows that have outlier values in \"glyburide\" column\n",
    "df = df[df['glyburide'] != 'Up']\n",
    "df = df[df['glyburide'] != 'Down']\n",
    "\n",
    "# Drop rows that have outlier values in \"pioglitazone\" column\n",
    "df = df[df['pioglitazone'] != 'Up']\n",
    "df = df[df['pioglitazone'] != 'Down']\n",
    "\n",
    "# Drop rows that have outlier values in \"rosiglitazone\" column\n",
    "df = df[df['rosiglitazone'] != 'Up']\n",
    "df = df[df['rosiglitazone'] != 'Down']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ DROP OUTLIERS ------\n",
    "\n",
    "# In \"number_outpatient\" column, limit the outliers (that have values from 6 to 40) to a maximum value of 5\n",
    "df['number_outpatient'] = df['number_outpatient'].apply(lambda x: x if x <= 5 else 5)\n",
    "\n",
    "# In \"number_emergency\" column, limit the outliers to a maximum value of 5\n",
    "df['number_emergency'] = df['number_emergency'].apply(lambda x: x if x <= 5 else 5)\n",
    "\n",
    "# In \"number_inpatient\" column, limit the outliers to a maximum value of 5\n",
    "df['number_inpatient'] = df['number_inpatient'].apply(lambda x: x if x <= 5 else 5)\n",
    "\n",
    "# In \"num_medications\" column, limit the outliers to a maximum value of 40\n",
    "df['num_medications'] = df['num_medications'].apply(lambda x: x if x <= 40 else 40)\n",
    "\n",
    "# In \"num_lab_procedures\" column, limit the outliers to a maximum value of 90\n",
    "df['num_lab_procedures'] = df['num_medications'].apply(lambda x: x if x <= 90 else 90)\n",
    "\n",
    "# In \"number_diagnoses\" column, limit the outliers to a maximum value of 9\n",
    "df['number_diagnoses'] = df['number_diagnoses'].apply(lambda x: x if x <= 9 else 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ TURN FEATURES INTO BINARY ------\n",
    "\n",
    "# Turn \"gender\" into binary (Female=0, Male=1)\n",
    "df['gender'] = df['gender'].replace({'Female': 0, 'Male': 1})\n",
    "\n",
    "# Turn \"metformin\" remaining values into binary (No=0, Steady=1)\n",
    "df['metformin'] = df['metformin'].replace({'No': 0, 'Steady': 1})\n",
    "\n",
    "# Turn \"glimepiride\" remaining values into binary (No=0, Steady=1)\n",
    "df['glimepiride'] = df['glimepiride'].replace({'No': 0, 'Steady': 1})\n",
    "\n",
    "# Turn \"glipizide\" remaining values into binary (No=0, Steady=1)\n",
    "df['glipizide'] = df['glipizide'].replace({'No': 0, 'Steady': 1})\n",
    "\n",
    "# Turn \"glyburide\" remaining values into binary (No=0, Steady=1)\n",
    "df['glyburide'] = df['glyburide'].replace({'No': 0, 'Steady': 1})\n",
    "\n",
    "# Turn \"pioglitazone\" remaining values into binary (No=0, Steady=1)\n",
    "df['pioglitazone'] = df['pioglitazone'].replace({'No': 0, 'Steady': 1})\n",
    "\n",
    "# Turn \"rosiglitazone\" remaining values into binary (No=0, Steady=1)\n",
    "df['rosiglitazone'] = df['rosiglitazone'].replace({'No': 0, 'Steady': 1})\n",
    "\n",
    "# Turn \"change\" into binary (No=0, Ch=1)\n",
    "df['change'] = df['change'].replace({'No': 0, 'Ch': 1})\n",
    "\n",
    "# Turn \"diabetesMed\" into binary (No=0, Yes=1)\n",
    "df['diabetesMed'] = df['diabetesMed'].replace({'No': 0, 'Yes': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ TURN CATEGORICAL FEATURES INTO NUMERIC, WITH ORDINAL ENCODING ------\n",
    "# Turn \"age\" into ordinal encoding\n",
    "age_mapping = {'[0-10)': 0,\n",
    "               '[10-20)': 1,\n",
    "               '[20-30)': 2,\n",
    "               '[30-40)': 3,\n",
    "               '[40-50)': 4,\n",
    "               '[50-60)': 5,\n",
    "               '[60-70)': 6,\n",
    "               '[70-80)': 7,\n",
    "               '[80-90)': 8,\n",
    "               '[90-100)': 9}\n",
    "df['age'] = df['age'].replace(age_mapping)\n",
    "\n",
    "# Turn \"max_glu_serum\" into ordinal encoding\n",
    "df['max_glu_serum'].fillna(0, inplace=True)\n",
    "df['max_glu_serum'] = df['max_glu_serum'].replace({'Norm': 0, '>200': 1, '>300': 2})\n",
    "\n",
    "# Turn \"A1Cresult\" into ordinal encoding\n",
    "df['A1Cresult'].fillna(0, inplace=True)\n",
    "df['A1Cresult'] = df['A1Cresult'].replace({'Norm': 0, '>7': 1, '>8': 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ TURN CATEGORICAL FEATURES INTO NUMERIC, WITH DUMMY ENCODING ------\n",
    "\n",
    "# Turn \"race\" into dummy encoding (Caucasian, AfricanAmerican, Hispanic, Other, Asian)\n",
    "df = pd.get_dummies(df, columns=['race'])\n",
    "\n",
    "# Turn \"insulin\" into dummy encoding (No, Steady, Up, Down)\n",
    "df = pd.get_dummies(df, columns=['insulin'])\n",
    "\n",
    "# In \"admission_type_id\" column, merge/simplify values into fewer categories (from 1-8 to 1,3,4,5) and then turn it\n",
    "# into dummy encoding (1: Emergency, 3: Elective, 4: Newborn, 5: Other)\n",
    "df['admission_type_id'] = df['admission_type_id'].replace(2,1)\n",
    "df['admission_type_id'] = df['admission_type_id'].replace(7,1)\n",
    "df['admission_type_id'] = df['admission_type_id'].replace(6,5)\n",
    "df['admission_type_id'] = df['admission_type_id'].replace(8,5)\n",
    "df['admission_type_id'] = df['admission_type_id'].replace({'1': \"Emergency\", '3': \"Elective\", '4': \"Newborn\", '5': \"Other\"})\n",
    "df = pd.get_dummies(df, columns=['admission_type_id'])\n",
    "\n",
    "# In \"admission_source_id\" column, merge/simplify values into fewer categories and then turn it\n",
    "# into dummy encoding (1: Physician Referral, 4: Transfer from a hospital, 9: Transfer from a Skilled Nursing Facility, 11: Other)\n",
    "df['admission_source_id'] = df['admission_source_id'].replace(2,1)\n",
    "df['admission_source_id'] = df['admission_source_id'].replace(3,1)\n",
    "df['admission_source_id'] = df['admission_source_id'].replace(5,4)\n",
    "df['admission_source_id'] = df['admission_source_id'].replace(6,4)\n",
    "df['admission_source_id'] = df['admission_source_id'].replace(10,4)\n",
    "df['admission_source_id'] = df['admission_source_id'].replace(22,4)\n",
    "df['admission_source_id'] = df['admission_source_id'].replace(25,4)\n",
    "df['admission_source_id'] = df['admission_source_id'].replace(15,9)\n",
    "df['admission_source_id'] = df['admission_source_id'].replace(17,9)\n",
    "df['admission_source_id'] = df['admission_source_id'].replace(20,9)\n",
    "df['admission_source_id'] = df['admission_source_id'].replace(21,9)\n",
    "df['admission_source_id'] = df['admission_source_id'].replace(8,11)\n",
    "df['admission_source_id'] = df['admission_source_id'].replace(13,11)\n",
    "df['admission_source_id'] = df['admission_source_id'].replace(14,11)\n",
    "df['admission_source_id'] = df['admission_source_id'].replace({'1': \"Physician_Referral\", '4': \"Transfer_Hospital\", '7': \"Emergency_Room\", '9': \"Transfer_Nursing\", '11': \"Other\"})\n",
    "df = pd.get_dummies(df, columns=['admission_source_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ TURN TARGET FEATURE INTO BINARY FOR FIRST PREDICTION: READMISION ------\n",
    "# Turn \"readmitted\" into binary (No=0, <30=1, >30=1) in a copy of the dataframe\n",
    "df1 = df.copy()\n",
    "df1['readmitted'] = df1['readmitted'].replace({'NO': 0, '<30': 1, '>30': 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Splitting & normalizing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ SPLIT PREPARED DATASET INTO: X and y  ------\n",
    "X = df1.drop('readmitted', axis=1)\n",
    "y = df1['readmitted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ SPLIT DATASET INTO TRAIN AND TEST ------\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "readmitted\n",
       "0    45614\n",
       "1    39427\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ NORMALIZE PREPARED DATASET ------\n",
    "# Normalize the dataset using MinMaxScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Training & evaluating multi-classification models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: 0.618201\n",
      "LDA: 0.618095\n",
      "Random Forest: 0.633122\n",
      "Decision Tree: 0.555873\n",
      "KNN: 0.559153\n",
      "Naive Bayes: 0.607196\n"
     ]
    }
   ],
   "source": [
    "# Import multi-classification models\n",
    "from sklearn.linear_model import LogisticRegression # Use OVR strategy\n",
    "from sklearn.svm import SVC # Use OVR strategy\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n",
    "# Define the models\n",
    "models = []\n",
    "# models.append(('Logistic Regression', LogisticRegression(multi_class='ovr', solver='liblinear')))\n",
    "models.append(('Logistic Regression', LogisticRegression()))\n",
    "# models.append(('SVM', OneVsRestClassifier(SVC())))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('Random Forest', RandomForestClassifier()))\n",
    "# models.append(('XGBoost', XGBClassifier()))\n",
    "models.append(('Decision Tree', DecisionTreeClassifier()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('Naive Bayes', GaussianNB()))\n",
    "\n",
    "\n",
    "# Train and evaluate each model\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "    # kfold = StratifiedKFold(n_splits=2, random_state=1, shuffle=True)\n",
    "    # cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring='accuracy')\n",
    "    cv_results = model.fit(X_train, y_train).score(X_test, y_test)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    print('%s: %f' % (name, cv_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ TURN TARGET FEATURE INTO BINARY FOR SECOND PREDICTION: READMISION <30 ------\n",
    "# Turn \"readmitted\" into binary (No=0, <30=1, >30=0) in a copy of the dataframe\n",
    "df2 = df.copy()\n",
    "df2['readmitted'] = df2['readmitted'].replace({'NO': 0, '<30': 1, '>30': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ SPLIT PREPARED DATASET INTO: X and y  ------\n",
    "X = df2.drop('readmitted', axis=1)\n",
    "y = df2['readmitted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ SPLIT DATASET INTO TRAIN AND TEST ------\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "readmitted\n",
       "0    75548\n",
       "1     9493\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ------ RE BALANCING DATA ------\n",
    "y_train.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ RE BALANCING DATA ------\n",
    "y_train.value_counts()\n",
    "from imblearn.over_sampling import SMOTE\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train, y_train = sm.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "readmitted\n",
       "0    8338\n",
       "1    1112\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ NORMALIZE PREPARED DATASET ------\n",
    "# Normalize the dataset using MinMaxScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: 0.880423\n",
      "LDA: 0.881376\n",
      "Random Forest: 0.880847\n",
      "Decision Tree: 0.782116\n",
      "KNN: 0.828360\n",
      "Naive Bayes: 0.517884\n"
     ]
    }
   ],
   "source": [
    "# Import multi-classification models\n",
    "from sklearn.linear_model import LogisticRegression # Use OVR strategy\n",
    "from sklearn.svm import SVC # Use OVR strategy\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n",
    "# Define the models\n",
    "models = []\n",
    "# models.append(('Logistic Regression', LogisticRegression(multi_class='ovr', solver='liblinear')))\n",
    "models.append(('Logistic Regression', LogisticRegression()))\n",
    "# models.append(('SVM', OneVsRestClassifier(SVC())))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('Random Forest', RandomForestClassifier()))\n",
    "# models.append(('XGBoost', XGBClassifier()))\n",
    "models.append(('Decision Tree', DecisionTreeClassifier()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('Naive Bayes', GaussianNB()))\n",
    "\n",
    "\n",
    "# Train and evaluate each model\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "    # kfold = StratifiedKFold(n_splits=2, random_state=1, shuffle=True)\n",
    "    # cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring='accuracy')\n",
    "    cv_results = model.fit(X_train, y_train).score(X_test, y_test)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    print('%s: %f' % (name, cv_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "[[8294   44]\n",
      " [1086   26]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.99      0.94      8338\n",
      "           1       0.37      0.02      0.04      1112\n",
      "\n",
      "    accuracy                           0.88      9450\n",
      "   macro avg       0.63      0.51      0.49      9450\n",
      "weighted avg       0.82      0.88      0.83      9450\n",
      "\n",
      "Accuracy:  0.8804232804232804\n",
      "Precision:  0.37142857142857144\n",
      "Recall:  0.023381294964028777\n",
      "F1 Score:  0.043993231810490696\n",
      "ROC AUC:  0.5090521250545738\n",
      "\n",
      "LDA\n",
      "[[8320   18]\n",
      " [1103    9]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      1.00      0.94      8338\n",
      "           1       0.33      0.01      0.02      1112\n",
      "\n",
      "    accuracy                           0.88      9450\n",
      "   macro avg       0.61      0.50      0.48      9450\n",
      "weighted avg       0.82      0.88      0.83      9450\n",
      "\n",
      "Accuracy:  0.8813756613756614\n",
      "Precision:  0.3333333333333333\n",
      "Recall:  0.008093525179856115\n",
      "F1 Score:  0.015803336259877086\n",
      "ROC AUC:  0.5029673670514296\n",
      "\n",
      "Random Forest\n",
      "[[8302   36]\n",
      " [1090   22]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      1.00      0.94      8338\n",
      "           1       0.38      0.02      0.04      1112\n",
      "\n",
      "    accuracy                           0.88      9450\n",
      "   macro avg       0.63      0.51      0.49      9450\n",
      "weighted avg       0.82      0.88      0.83      9450\n",
      "\n",
      "Accuracy:  0.8808465608465609\n",
      "Precision:  0.3793103448275862\n",
      "Recall:  0.019784172661870502\n",
      "F1 Score:  0.037606837606837605\n",
      "ROC AUC:  0.5077332952539384\n",
      "\n",
      "Decision Tree\n",
      "[[7187 1151]\n",
      " [ 908  204]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.86      0.87      8338\n",
      "           1       0.15      0.18      0.17      1112\n",
      "\n",
      "    accuracy                           0.78      9450\n",
      "   macro avg       0.52      0.52      0.52      9450\n",
      "weighted avg       0.80      0.78      0.79      9450\n",
      "\n",
      "Accuracy:  0.7821164021164021\n",
      "Precision:  0.15055350553505534\n",
      "Recall:  0.18345323741007194\n",
      "F1 Score:  0.16538305634373734\n",
      "ROC AUC:  0.5227052706599412\n",
      "\n",
      "KNN\n",
      "[[7686  652]\n",
      " [ 970  142]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.92      0.90      8338\n",
      "           1       0.18      0.13      0.15      1112\n",
      "\n",
      "    accuracy                           0.83      9450\n",
      "   macro avg       0.53      0.52      0.53      9450\n",
      "weighted avg       0.80      0.83      0.82      9450\n",
      "\n",
      "Accuracy:  0.8283597883597884\n",
      "Precision:  0.17884130982367757\n",
      "Recall:  0.12769784172661872\n",
      "F1 Score:  0.14900314795383002\n",
      "ROC AUC:  0.5247508158021437\n",
      "\n",
      "Naive Bayes\n",
      "[[4332 4006]\n",
      " [ 550  562]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.52      0.66      8338\n",
      "           1       0.12      0.51      0.20      1112\n",
      "\n",
      "    accuracy                           0.52      9450\n",
      "   macro avg       0.51      0.51      0.43      9450\n",
      "weighted avg       0.80      0.52      0.60      9450\n",
      "\n",
      "Accuracy:  0.5178835978835978\n",
      "Precision:  0.12302977232924693\n",
      "Recall:  0.5053956834532374\n",
      "F1 Score:  0.19788732394366196\n",
      "ROC AUC:  0.5124723679919101\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "for name, model in models:\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(name)\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "    print('Precision: ', precision_score(y_test, y_pred))\n",
    "    print('Recall: ', recall_score(y_test, y_pred))\n",
    "    print('F1 Score: ', f1_score(y_test, y_pred))\n",
    "    print('ROC AUC: ', roc_auc_score(y_test, y_pred))\n",
    "    print('')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
