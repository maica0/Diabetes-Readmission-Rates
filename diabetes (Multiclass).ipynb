{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML GROUP ASSIGNMENT:\n",
    "### Predicting readmissions by leveraging \"diabetic_data.csv\"\n",
    "MBD Section 2, Group 6\n",
    "\n",
    "Group members & notebook credit:\n",
    "- Ignacio Ferro\n",
    "- Jose Carranque\n",
    "- Maica Muñoz\n",
    "- Maria Jose Perez\n",
    "- Mohammed Alotaibi\n",
    "- Rodrigo Reyes Sanchez\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0 - Loading key libraries & dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101766, 50)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#Loading Dataset\n",
    "df = pd.read_csv(\"diabetic_readmission_data.csv\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "race 2273\n",
      "gender 0\n",
      "age 0\n",
      "weight 98569\n",
      "payer_code 40256\n",
      "medical_specialty 49949\n",
      "diag_1 21\n",
      "diag_2 358\n",
      "diag_3 1423\n",
      "max_glu_serum 0\n",
      "A1Cresult 0\n",
      "metformin 0\n",
      "repaglinide 0\n",
      "nateglinide 0\n",
      "chlorpropamide 0\n",
      "glimepiride 0\n",
      "acetohexamide 0\n",
      "glipizide 0\n",
      "glyburide 0\n",
      "tolbutamide 0\n",
      "pioglitazone 0\n",
      "rosiglitazone 0\n",
      "acarbose 0\n",
      "miglitol 0\n",
      "troglitazone 0\n",
      "tolazamide 0\n",
      "examide 0\n",
      "citoglipton 0\n",
      "insulin 0\n",
      "glyburide-metformin 0\n",
      "glipizide-metformin 0\n",
      "glimepiride-pioglitazone 0\n",
      "metformin-rosiglitazone 0\n",
      "metformin-pioglitazone 0\n",
      "change 0\n",
      "diabetesMed 0\n",
      "readmitted 0\n"
     ]
    }
   ],
   "source": [
    "df.isna().sum()\n",
    "\n",
    "#Identify number of missing values (represented as '?' sign) for each feature\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == object:\n",
    "         print(col,df[col][df[col] == '?'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "race 2273\n",
      "gender 0\n",
      "age 0\n",
      "weight 98569\n",
      "payer_code 40256\n",
      "medical_specialty 49949\n",
      "diag_1 21\n",
      "diag_2 358\n",
      "diag_3 1423\n",
      "max_glu_serum 0\n",
      "A1Cresult 0\n",
      "metformin 0\n",
      "repaglinide 0\n",
      "nateglinide 0\n",
      "chlorpropamide 0\n",
      "glimepiride 0\n",
      "acetohexamide 0\n",
      "glipizide 0\n",
      "glyburide 0\n",
      "tolbutamide 0\n",
      "pioglitazone 0\n",
      "rosiglitazone 0\n",
      "acarbose 0\n",
      "miglitol 0\n",
      "troglitazone 0\n",
      "tolazamide 0\n",
      "examide 0\n",
      "citoglipton 0\n",
      "insulin 0\n",
      "glyburide-metformin 0\n",
      "glipizide-metformin 0\n",
      "glimepiride-pioglitazone 0\n",
      "metformin-rosiglitazone 0\n",
      "metformin-pioglitazone 0\n",
      "change 0\n",
      "diabetesMed 0\n",
      "readmitted 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "readmitted\n",
       "NO     54864\n",
       ">30    35545\n",
       "<30    11357\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify number of null values for each feature\n",
    "df.isna().sum()\n",
    "\n",
    "#Identify number of missing values (represented as '?' sign) for each feature\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == object:\n",
    "         print(col,df[col][df[col] == '?'].count())\n",
    "\n",
    "# Analyze the distribution of values for Numerical features (identify outliers)\n",
    "df['time_in_hospital'].describe()\n",
    "df['num_lab_procedures'].describe()\n",
    "df['num_medications'].describe()\n",
    "df['number_outpatient'].describe()\n",
    "df['number_emergency'].describe()\n",
    "df['number_inpatient'].describe()\n",
    "df['number_diagnoses'].describe()\n",
    "\n",
    "# Analyze the distribution of values for Categorical features (identify outliers)\n",
    "df['race'].value_counts()\n",
    "df['gender'].value_counts()\n",
    "df['age'].value_counts()\n",
    "df['weight'].value_counts()\n",
    "df['num_procedures'].value_counts()\n",
    "df['admission_type_id'].value_counts()\n",
    "df['discharge_disposition_id'].value_counts()\n",
    "df['admission_source_id'].value_counts()\n",
    "df['payer_code'].value_counts()\n",
    "df['medical_specialty'].value_counts()\n",
    "df['diag_1'].value_counts()\n",
    "df['diag_2'].value_counts()\n",
    "df['diag_3'].value_counts()\n",
    "df['max_glu_serum'].value_counts()\n",
    "df['A1Cresult'].value_counts()\n",
    "df['metformin'].value_counts()\n",
    "df['repaglinide'].value_counts()\n",
    "df['nateglinide'].value_counts()\n",
    "df['chlorpropamide'].value_counts()\n",
    "df['glimepiride'].value_counts()\n",
    "df['acetohexamide'].value_counts()\n",
    "df['glipizide'].value_counts()\n",
    "df['glyburide'].value_counts()\n",
    "df['tolbutamide'].value_counts()\n",
    "df['pioglitazone'].value_counts()\n",
    "df['rosiglitazone'].value_counts()\n",
    "df['acarbose'].value_counts()\n",
    "df['miglitol'].value_counts()\n",
    "df['troglitazone'].value_counts()\n",
    "df['tolazamide'].value_counts()\n",
    "df['examide'].value_counts()\n",
    "df['citoglipton'].value_counts()\n",
    "df['insulin'].value_counts()\n",
    "df['glyburide-metformin'].value_counts()\n",
    "df['glipizide-metformin'].value_counts()\n",
    "df['glimepiride-pioglitazone'].value_counts()\n",
    "df['metformin-rosiglitazone'].value_counts()\n",
    "df['metformin-pioglitazone'].value_counts()\n",
    "df['change'].value_counts()\n",
    "df['diabetesMed'].value_counts()\n",
    "df['num_procedures'].value_counts()\n",
    "df['readmitted'].value_counts()\n",
    "\n",
    "\n",
    "# ---- CODE TO PLOT HISTOGRAMS ----\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# plt.hist(df['num_medications'], color='skyblue', edgecolor='black')\n",
    "# plt.xlabel('Number of Medications')\n",
    "# plt.ylabel('Frequency')\n",
    "# plt.title('Distribution of Number of Medications')\n",
    "# plt.grid(True)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Data Cleaning & Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ CREATE NEW COLUMNS ------\n",
    "\n",
    "# Create a new column called \"encounter_number\".\n",
    "# It counts the number of encounters that each unique \"patient_nbr\" did until that moment (ordinal encoding)\n",
    "df['encounter_number'] = df.groupby('patient_nbr').cumcount() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ SIMPLIFICATION OF diag_1, diag_2 and diag_3 ------\n",
    "# Source & credit: https://www.kaggle.com/code/iabhishekofficial/prediction-on-hospital-readmission?scriptVersionId=14883095&cellId=35\n",
    "\n",
    "# Categorization of diagnoses: The dataset contained up to three diagnoses for a given patient (primary, secondary and additional).\n",
    "# However, each of these had 700–900 unique ICD codes and it is extremely difficult to include them in the model and interpret meaningfully.\n",
    "# Therefore, we collapsed these diagnosis codes into 9 disease categories in an almost similar fashion to that done in the original\n",
    "# publication using this dataset.\n",
    "# These 9 categories include Circulatory, Respiratory, Digestive, Diabetes, Injury, Musculoskeletal, Genitourinary, Neoplasms, and Others.\n",
    "\n",
    "# Although we did this for primary, secondary and additional diagnoses, we eventually decided to use only the primary diagnosis in our model.\n",
    "# Doing this in python was slightly cumbersome because, well, we are mapping the disease codes to certain category names.\n",
    "# Below code should demonstrate this easily.\n",
    "\n",
    "\n",
    "# Creating additional columns for diagnosis\n",
    "df['level1_diag1'] = df['diag_1']\n",
    "df['level2_diag1'] = df['diag_1']\n",
    "df['level1_diag2'] = df['diag_2']\n",
    "df['level2_diag2'] = df['diag_2']\n",
    "df['level1_diag3'] = df['diag_3']\n",
    "df['level2_diag3'] = df['diag_3']\n",
    "\n",
    "df.loc[df['diag_1'].str.contains('V'), ['level1_diag1', 'level2_diag1']] = 0\n",
    "df.loc[df['diag_1'].str.contains('E'), ['level1_diag1', 'level2_diag1']] = 0\n",
    "df.loc[df['diag_2'].str.contains('V'), ['level1_diag2', 'level2_diag2']] = 0\n",
    "df.loc[df['diag_2'].str.contains('E'), ['level1_diag2', 'level2_diag2']] = 0\n",
    "df.loc[df['diag_3'].str.contains('V'), ['level1_diag3', 'level2_diag3']] = 0\n",
    "df.loc[df['diag_3'].str.contains('E'), ['level1_diag3', 'level2_diag3']] = 0\n",
    "df['level1_diag1'] = df['level1_diag1'].replace('?', -1)\n",
    "df['level2_diag1'] = df['level2_diag1'].replace('?', -1)\n",
    "df['level1_diag2'] = df['level1_diag2'].replace('?', -1)\n",
    "df['level2_diag2'] = df['level2_diag2'].replace('?', -1)\n",
    "df['level1_diag3'] = df['level1_diag3'].replace('?', -1)\n",
    "df['level2_diag3'] = df['level2_diag3'].replace('?', -1)\n",
    "\n",
    "df['level1_diag1'] = df['level1_diag1'].astype(float)\n",
    "df['level2_diag1'] = df['level2_diag1'].astype(float)\n",
    "df['level1_diag2'] = df['level1_diag2'].astype(float)\n",
    "df['level2_diag2'] = df['level2_diag2'].astype(float)\n",
    "df['level1_diag3'] = df['level1_diag3'].astype(float)\n",
    "df['level2_diag3'] = df['level2_diag3'].astype(float)\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if (row['level1_diag1'] >= 390 and row['level1_diag1'] < 460) or (np.floor(row['level1_diag1']) == 785):\n",
    "        df.loc[index, 'level1_diag1'] = 1\n",
    "    elif (row['level1_diag1'] >= 460 and row['level1_diag1'] < 520) or (np.floor(row['level1_diag1']) == 786):\n",
    "        df.loc[index, 'level1_diag1'] = 2\n",
    "    elif (row['level1_diag1'] >= 520 and row['level1_diag1'] < 580) or (np.floor(row['level1_diag1']) == 787):\n",
    "        df.loc[index, 'level1_diag1'] = 3\n",
    "    elif (np.floor(row['level1_diag1']) == 250):\n",
    "        df.loc[index, 'level1_diag1'] = 4\n",
    "    elif (row['level1_diag1'] >= 800 and row['level1_diag1'] < 1000):\n",
    "        df.loc[index, 'level1_diag1'] = 5\n",
    "    elif (row['level1_diag1'] >= 710 and row['level1_diag1'] < 740):\n",
    "        df.loc[index, 'level1_diag1'] = 6\n",
    "    elif (row['level1_diag1'] >= 580 and row['level1_diag1'] < 630) or (np.floor(row['level1_diag1']) == 788):\n",
    "        df.loc[index, 'level1_diag1'] = 7\n",
    "    elif (row['level1_diag1'] >= 140 and row['level1_diag1'] < 240):\n",
    "        df.loc[index, 'level1_diag1'] = 8\n",
    "    else:\n",
    "        df.loc[index, 'level1_diag1'] = 0\n",
    "        \n",
    "    if (row['level1_diag2'] >= 390 and row['level1_diag2'] < 460) or (np.floor(row['level1_diag2']) == 785):\n",
    "        df.loc[index, 'level1_diag2'] = 1\n",
    "    elif (row['level1_diag2'] >= 460 and row['level1_diag2'] < 520) or (np.floor(row['level1_diag2']) == 786):\n",
    "        df.loc[index, 'level1_diag2'] = 2\n",
    "    elif (row['level1_diag2'] >= 520 and row['level1_diag2'] < 580) or (np.floor(row['level1_diag2']) == 787):\n",
    "        df.loc[index, 'level1_diag2'] = 3\n",
    "    elif (np.floor(row['level1_diag2']) == 250):\n",
    "        df.loc[index, 'level1_diag2'] = 4\n",
    "    elif (row['level1_diag2'] >= 800 and row['level1_diag2'] < 1000):\n",
    "        df.loc[index, 'level1_diag2'] = 5\n",
    "    elif (row['level1_diag2'] >= 710 and row['level1_diag2'] < 740):\n",
    "        df.loc[index, 'level1_diag2'] = 6\n",
    "    elif (row['level1_diag2'] >= 580 and row['level1_diag2'] < 630) or (np.floor(row['level1_diag2']) == 788):\n",
    "        df.loc[index, 'level1_diag2'] = 7\n",
    "    elif (row['level1_diag2'] >= 140 and row['level1_diag2'] < 240):\n",
    "        df.loc[index, 'level1_diag2'] = 8\n",
    "    else:\n",
    "        df.loc[index, 'level1_diag2'] = 0\n",
    "    \n",
    "    if (row['level1_diag3'] >= 390 and row['level1_diag3'] < 460) or (np.floor(row['level1_diag3']) == 785):\n",
    "        df.loc[index, 'level1_diag3'] = 1\n",
    "    elif (row['level1_diag3'] >= 460 and row['level1_diag3'] < 520) or (np.floor(row['level1_diag3']) == 786):\n",
    "        df.loc[index, 'level1_diag3'] = 2\n",
    "    elif (row['level1_diag3'] >= 520 and row['level1_diag3'] < 580) or (np.floor(row['level1_diag3']) == 787):\n",
    "        df.loc[index, 'level1_diag3'] = 3\n",
    "    elif (np.floor(row['level1_diag3']) == 250):\n",
    "        df.loc[index, 'level1_diag3'] = 4\n",
    "    elif (row['level1_diag3'] >= 800 and row['level1_diag3'] < 1000):\n",
    "        df.loc[index, 'level1_diag3'] = 5\n",
    "    elif (row['level1_diag3'] >= 710 and row['level1_diag3'] < 740):\n",
    "        df.loc[index, 'level1_diag3'] = 6\n",
    "    elif (row['level1_diag3'] >= 580 and row['level1_diag3'] < 630) or (np.floor(row['level1_diag3']) == 788):\n",
    "        df.loc[index, 'level1_diag3'] = 7\n",
    "    elif (row['level1_diag3'] >= 140 and row['level1_diag3'] < 240):\n",
    "        df.loc[index, 'level1_diag3'] = 8\n",
    "    else:\n",
    "        df.loc[index, 'level1_diag3'] = 0\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if (row['level2_diag1'] >= 390 and row['level2_diag1'] < 399):\n",
    "        df.loc[index, 'level2_diag1'] = 1\n",
    "    elif (row['level2_diag1'] >= 401 and row['level2_diag1'] < 415):\n",
    "        df.loc[index, 'level2_diag1'] = 2\n",
    "    elif (row['level2_diag1'] >= 415 and row['level2_diag1'] < 460):\n",
    "        df.loc[index, 'level2_diag1'] = 3\n",
    "    elif (np.floor(row['level2_diag1']) == 785):\n",
    "        df.loc[index, 'level2_diag1'] = 4\n",
    "    elif (row['level2_diag1'] >= 460 and row['level2_diag1'] < 489):\n",
    "        df.loc[index, 'level2_diag1'] = 5\n",
    "    elif (row['level2_diag1'] >= 490 and row['level2_diag1'] < 497):\n",
    "        df.loc[index, 'level2_diag1'] = 6\n",
    "    elif (row['level2_diag1'] >= 500 and row['level2_diag1'] < 520):\n",
    "        df.loc[index, 'level2_diag1'] = 7\n",
    "    elif (np.floor(row['level2_diag1']) == 786):\n",
    "        df.loc[index, 'level2_diag1'] = 8\n",
    "    elif (row['level2_diag1'] >= 520 and row['level2_diag1'] < 530):\n",
    "        df.loc[index, 'level2_diag1'] = 9\n",
    "    elif (row['level2_diag1'] >= 530 and row['level2_diag1'] < 544):\n",
    "        df.loc[index, 'level2_diag1'] = 10\n",
    "    elif (row['level2_diag1'] >= 550 and row['level2_diag1'] < 554):\n",
    "        df.loc[index, 'level2_diag1'] = 11\n",
    "    elif (row['level2_diag1'] >= 555 and row['level2_diag1'] < 580):\n",
    "        df.loc[index, 'level2_diag1'] = 12\n",
    "    elif (np.floor(row['level2_diag1']) == 787):\n",
    "        df.loc[index, 'level2_diag1'] = 13\n",
    "    elif (np.floor(row['level2_diag1']) == 250):\n",
    "        df.loc[index, 'level2_diag1'] = 14\n",
    "    elif (row['level2_diag1'] >= 800 and row['level2_diag1'] < 1000):\n",
    "        df.loc[index, 'level2_diag1'] = 15\n",
    "    elif (row['level2_diag1'] >= 710 and row['level2_diag1'] < 740):\n",
    "        df.loc[index, 'level2_diag1'] = 16\n",
    "    elif (row['level2_diag1'] >= 580 and row['level2_diag1'] < 630):\n",
    "        df.loc[index, 'level2_diag1'] = 17\n",
    "    elif (np.floor(row['level2_diag1']) == 788):\n",
    "        df.loc[index, 'level2_diag1'] = 18\n",
    "    elif (row['level2_diag1'] >= 140 and row['level2_diag1'] < 240):\n",
    "        df.loc[index, 'level2_diag1'] = 19\n",
    "    elif row['level2_diag1'] >= 240 and row['level2_diag1'] < 280 and (np.floor(row['level2_diag1']) != 250):\n",
    "        df.loc[index, 'level2_diag1'] = 20\n",
    "    elif (row['level2_diag1'] >= 680 and row['level2_diag1'] < 710) or (np.floor(row['level2_diag1']) == 782):\n",
    "        df.loc[index, 'level2_diag1'] = 21\n",
    "    elif (row['level2_diag1'] >= 290 and row['level2_diag1'] < 320):\n",
    "        df.loc[index, 'level2_diag1'] = 22\n",
    "    else:\n",
    "        df.loc[index, 'level2_diag1'] = 0\n",
    "        \n",
    "    if (row['level2_diag2'] >= 390 and row['level2_diag2'] < 399):\n",
    "        df.loc[index, 'level2_diag2'] = 1\n",
    "    elif (row['level2_diag2'] >= 401 and row['level2_diag2'] < 415):\n",
    "        df.loc[index, 'level2_diag2'] = 2\n",
    "    elif (row['level2_diag2'] >= 415 and row['level2_diag2'] < 460):\n",
    "        df.loc[index, 'level2_diag2'] = 3\n",
    "    elif (np.floor(row['level2_diag2']) == 785):\n",
    "        df.loc[index, 'level2_diag2'] = 4\n",
    "    elif (row['level2_diag2'] >= 460 and row['level2_diag2'] < 489):\n",
    "        df.loc[index, 'level2_diag2'] = 5\n",
    "    elif (row['level2_diag2'] >= 490 and row['level2_diag2'] < 497):\n",
    "        df.loc[index, 'level2_diag2'] = 6\n",
    "    elif (row['level2_diag2'] >= 500 and row['level2_diag2'] < 520):\n",
    "        df.loc[index, 'level2_diag2'] = 7\n",
    "    elif (np.floor(row['level2_diag2']) == 786):\n",
    "        df.loc[index, 'level2_diag2'] = 8\n",
    "    elif (row['level2_diag2'] >= 520 and row['level2_diag2'] < 530):\n",
    "        df.loc[index, 'level2_diag2'] = 9\n",
    "    elif (row['level2_diag2'] >= 530 and row['level2_diag2'] < 544):\n",
    "        df.loc[index, 'level2_diag2'] = 10\n",
    "    elif (row['level2_diag2'] >= 550 and row['level2_diag2'] < 554):\n",
    "        df.loc[index, 'level2_diag2'] = 11\n",
    "    elif (row['level2_diag2'] >= 555 and row['level2_diag2'] < 580):\n",
    "        df.loc[index, 'level2_diag2'] = 12\n",
    "    elif (np.floor(row['level2_diag2']) == 787):\n",
    "        df.loc[index, 'level2_diag2'] = 13\n",
    "    elif (np.floor(row['level2_diag2']) == 250):\n",
    "        df.loc[index, 'level2_diag2'] = 14\n",
    "    elif (row['level2_diag2'] >= 800 and row['level2_diag2'] < 1000):\n",
    "        df.loc[index, 'level2_diag2'] = 15\n",
    "    elif (row['level2_diag2'] >= 710 and row['level2_diag2'] < 740):\n",
    "        df.loc[index, 'level2_diag2'] = 16\n",
    "    elif (row['level2_diag2'] >= 580 and row['level2_diag2'] < 630):\n",
    "        df.loc[index, 'level2_diag2'] = 17\n",
    "    elif (np.floor(row['level2_diag2']) == 788):\n",
    "        df.loc[index, 'level2_diag2'] = 18\n",
    "    elif (row['level2_diag2'] >= 140 and row['level2_diag2'] < 240):\n",
    "        df.loc[index, 'level2_diag2'] = 19\n",
    "    elif row['level2_diag2'] >= 240 and row['level2_diag2'] < 280 and (np.floor(row['level2_diag2']) != 250):\n",
    "        df.loc[index, 'level2_diag2'] = 20\n",
    "    elif (row['level2_diag2'] >= 680 and row['level2_diag2'] < 710) or (np.floor(row['level2_diag2']) == 782):\n",
    "        df.loc[index, 'level2_diag2'] = 21\n",
    "    elif (row['level2_diag2'] >= 290 and row['level2_diag2'] < 320):\n",
    "        df.loc[index, 'level2_diag2'] = 22\n",
    "    else:\n",
    "        df.loc[index, 'level2_diag2'] = 0\n",
    "        \n",
    "        \n",
    "    if (row['level2_diag3'] >= 390 and row['level2_diag3'] < 399):\n",
    "        df.loc[index, 'level2_diag3'] = 1\n",
    "    elif (row['level2_diag3'] >= 401 and row['level2_diag3'] < 415):\n",
    "        df.loc[index, 'level2_diag3'] = 2\n",
    "    elif (row['level2_diag3'] >= 415 and row['level2_diag3'] < 460):\n",
    "        df.loc[index, 'level2_diag3'] = 3\n",
    "    elif (np.floor(row['level2_diag3']) == 785):\n",
    "        df.loc[index, 'level2_diag3'] = 4\n",
    "    elif (row['level2_diag3'] >= 460 and row['level2_diag3'] < 489):\n",
    "        df.loc[index, 'level2_diag3'] = 5\n",
    "    elif (row['level2_diag3'] >= 490 and row['level2_diag3'] < 497):\n",
    "        df.loc[index, 'level2_diag3'] = 6\n",
    "    elif (row['level2_diag3'] >= 500 and row['level2_diag3'] < 520):\n",
    "        df.loc[index, 'level2_diag3'] = 7\n",
    "    elif (np.floor(row['level2_diag3']) == 786):\n",
    "        df.loc[index, 'level2_diag3'] = 8\n",
    "    elif (row['level2_diag3'] >= 520 and row['level2_diag3'] < 530):\n",
    "        df.loc[index, 'level2_diag3'] = 9\n",
    "    elif (row['level2_diag3'] >= 530 and row['level2_diag3'] < 544):\n",
    "        df.loc[index, 'level2_diag3'] = 10\n",
    "    elif (row['level2_diag3'] >= 550 and row['level2_diag3'] < 554):\n",
    "        df.loc[index, 'level2_diag3'] = 11\n",
    "    elif (row['level2_diag3'] >= 555 and row['level2_diag3'] < 580):\n",
    "        df.loc[index, 'level2_diag3'] = 12\n",
    "    elif (np.floor(row['level2_diag3']) == 787):\n",
    "        df.loc[index, 'level2_diag3'] = 13\n",
    "    elif (np.floor(row['level2_diag3']) == 250):\n",
    "        df.loc[index, 'level2_diag3'] = 14\n",
    "    elif (row['level2_diag3'] >= 800 and row['level2_diag3'] < 1000):\n",
    "        df.loc[index, 'level2_diag3'] = 15\n",
    "    elif (row['level2_diag3'] >= 710 and row['level2_diag3'] < 740):\n",
    "        df.loc[index, 'level2_diag3'] = 16\n",
    "    elif (row['level2_diag3'] >= 580 and row['level2_diag3'] < 630):\n",
    "        df.loc[index, 'level2_diag3'] = 17\n",
    "    elif (np.floor(row['level2_diag3']) == 788):\n",
    "        df.loc[index, 'level2_diag3'] = 18\n",
    "    elif (row['level2_diag3'] >= 140 and row['level2_diag3'] < 240):\n",
    "        df.loc[index, 'level2_diag3'] = 19\n",
    "    elif row['level2_diag3'] >= 240 and row['level2_diag3'] < 280 and (np.floor(row['level2_diag3']) != 250):\n",
    "        df.loc[index, 'level2_diag3'] = 20\n",
    "    elif (row['level2_diag3'] >= 680 and row['level2_diag3'] < 710) or (np.floor(row['level2_diag3']) == 782):\n",
    "        df.loc[index, 'level2_diag3'] = 21\n",
    "    elif (row['level2_diag3'] >= 290 and row['level2_diag3'] < 320):\n",
    "        df.loc[index, 'level2_diag3'] = 22\n",
    "    else:\n",
    "        df.loc[index, 'level2_diag3'] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ DROP USELESS COLUMNS ------\n",
    "features_to_drop = ['encounter_id',\n",
    "                    'patient_nbr',\n",
    "                    'weight', \n",
    "                    'payer_code',\n",
    "                    'medical_specialty',\n",
    "                    'diag_1',\n",
    "                    'diag_2',\n",
    "                    'diag_3',\n",
    "                    'repaglinide',\n",
    "                    'nateglinide',\n",
    "                    'chlorpropamide',\n",
    "                    'acetohexamide',\n",
    "                    'tolbutamide',\n",
    "                    'acarbose',\n",
    "                    'miglitol',\n",
    "                    'troglitazone',\n",
    "                    'tolazamide',\n",
    "                    'examide',\n",
    "                    'citoglipton',\n",
    "                    'glyburide-metformin',\n",
    "                    'glipizide-metformin',\n",
    "                    'glimepiride-pioglitazone',\n",
    "                    'metformin-rosiglitazone',\n",
    "                    'metformin-pioglitazone']\n",
    "df = df.drop(features_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ DROP ROWS WITH MISSING VALUES AND/OR OUTLIERS (THAT ARE NOT CRITICAL FOR THE MODEL) ------\n",
    "\n",
    "# Drop rows that have useless values in \"discharge_disposition_id\" column\n",
    "# Values of 11, 13, 14, 19, 20, or 21 are related to death or hospice which mean these patients cannot be readmitted.\n",
    "df = df[df['discharge_disposition_id'] != '11']\n",
    "df = df[df['discharge_disposition_id'] != '13']\n",
    "df = df[df['discharge_disposition_id'] != '14']\n",
    "df = df[df['discharge_disposition_id'] != '19']\n",
    "df = df[df['discharge_disposition_id'] != '20']\n",
    "df = df[df['discharge_disposition_id'] != '21']\n",
    "\n",
    "# Remove rows that have missing values in \"race\" column\n",
    "df = df[df['race'] != '?']\n",
    "\n",
    "# Drop rows that have outlier values in \"gender\" column\n",
    "df = df[df['gender'] != 'Unknown/Invalid']\n",
    "\n",
    "# Drop rows that have outlier values in \"metformin\" column\n",
    "df = df[df['metformin'] != 'Up']\n",
    "df = df[df['metformin'] != 'Down']\n",
    "\n",
    "# Drop rows that have outlier values in \"glimepiride\" column\n",
    "df = df[df['glimepiride'] != 'Up']\n",
    "df = df[df['glimepiride'] != 'Down']\n",
    "\n",
    "# Drop rows that have outlier values in \"glipizide\" column\n",
    "df = df[df['glipizide'] != 'Up']\n",
    "df = df[df['glipizide'] != 'Down']\n",
    "\n",
    "# Drop rows that have outlier values in \"glyburide\" column\n",
    "df = df[df['glyburide'] != 'Up']\n",
    "df = df[df['glyburide'] != 'Down']\n",
    "\n",
    "# Drop rows that have outlier values in \"pioglitazone\" column\n",
    "df = df[df['pioglitazone'] != 'Up']\n",
    "df = df[df['pioglitazone'] != 'Down']\n",
    "\n",
    "# Drop rows that have outlier values in \"rosiglitazone\" column\n",
    "df = df[df['rosiglitazone'] != 'Up']\n",
    "df = df[df['rosiglitazone'] != 'Down']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ DROP OUTLIERS ------\n",
    "\n",
    "# In \"number_outpatient\" column, limit the outliers (that have values from 6 to 40) to a maximum value of 5\n",
    "df['number_outpatient'] = df['number_outpatient'].apply(lambda x: x if x <= 5 else 5)\n",
    "\n",
    "# In \"number_emergency\" column, limit the outliers to a maximum value of 5\n",
    "df['number_emergency'] = df['number_emergency'].apply(lambda x: x if x <= 5 else 5)\n",
    "\n",
    "# In \"number_inpatient\" column, limit the outliers to a maximum value of 5\n",
    "df['number_inpatient'] = df['number_inpatient'].apply(lambda x: x if x <= 5 else 5)\n",
    "\n",
    "# In \"num_medications\" column, limit the outliers to a maximum value of 40\n",
    "df['num_medications'] = df['num_medications'].apply(lambda x: x if x <= 40 else 40)\n",
    "\n",
    "# In \"num_lab_procedures\" column, limit the outliers to a maximum value of 90\n",
    "df['num_lab_procedures'] = df['num_medications'].apply(lambda x: x if x <= 90 else 90)\n",
    "\n",
    "# In \"number_diagnoses\" column, limit the outliers to a maximum value of 9\n",
    "df['number_diagnoses'] = df['number_diagnoses'].apply(lambda x: x if x <= 9 else 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ TURN FEATURES INTO BINARY ------\n",
    "\n",
    "# Turn \"gender\" into binary (Female=0, Male=1)\n",
    "df['gender'] = df['gender'].replace({'Female': 0, 'Male': 1})\n",
    "\n",
    "# Turn \"metformin\" remaining values into binary (No=0, Steady=1)\n",
    "df['metformin'] = df['metformin'].replace({'No': 0, 'Steady': 1})\n",
    "\n",
    "# Turn \"glimepiride\" remaining values into binary (No=0, Steady=1)\n",
    "df['glimepiride'] = df['glimepiride'].replace({'No': 0, 'Steady': 1})\n",
    "\n",
    "# Turn \"glipizide\" remaining values into binary (No=0, Steady=1)\n",
    "df['glipizide'] = df['glipizide'].replace({'No': 0, 'Steady': 1})\n",
    "\n",
    "# Turn \"glyburide\" remaining values into binary (No=0, Steady=1)\n",
    "df['glyburide'] = df['glyburide'].replace({'No': 0, 'Steady': 1})\n",
    "\n",
    "# Turn \"pioglitazone\" remaining values into binary (No=0, Steady=1)\n",
    "df['pioglitazone'] = df['pioglitazone'].replace({'No': 0, 'Steady': 1})\n",
    "\n",
    "# Turn \"rosiglitazone\" remaining values into binary (No=0, Steady=1)\n",
    "df['rosiglitazone'] = df['rosiglitazone'].replace({'No': 0, 'Steady': 1})\n",
    "\n",
    "# Turn \"change\" into binary (No=0, Ch=1)\n",
    "df['change'] = df['change'].replace({'No': 0, 'Ch': 1})\n",
    "\n",
    "# Turn \"diabetesMed\" into binary (No=0, Yes=1)\n",
    "df['diabetesMed'] = df['diabetesMed'].replace({'No': 0, 'Yes': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ TURN CATEGORICAL FEATURES INTO NUMERIC, WITH ORDINAL ENCODING ------\n",
    "# Turn \"age\" into ordinal encoding\n",
    "age_mapping = {'[0-10)': 0,\n",
    "               '[10-20)': 1,\n",
    "               '[20-30)': 2,\n",
    "               '[30-40)': 3,\n",
    "               '[40-50)': 4,\n",
    "               '[50-60)': 5,\n",
    "               '[60-70)': 6,\n",
    "               '[70-80)': 7,\n",
    "               '[80-90)': 8,\n",
    "               '[90-100)': 9}\n",
    "df['age'] = df['age'].replace(age_mapping)\n",
    "\n",
    "# Turn \"max_glu_serum\" into ordinal encoding\n",
    "df['max_glu_serum'].fillna(0, inplace=True)\n",
    "df['max_glu_serum'] = df['max_glu_serum'].replace({'Norm': 0, '>200': 1, '>300': 2})\n",
    "\n",
    "# Turn \"A1Cresult\" into ordinal encoding\n",
    "df['A1Cresult'].fillna(0, inplace=True)\n",
    "df['A1Cresult'] = df['A1Cresult'].replace({'Norm': 0, '>7': 1, '>8': 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ TURN CATEGORICAL FEATURES INTO NUMERIC, WITH DUMMY ENCODING ------\n",
    "\n",
    "# Turn \"race\" into dummy encoding (Caucasian, AfricanAmerican, Hispanic, Other, Asian)\n",
    "df = pd.get_dummies(df, columns=['race'])\n",
    "\n",
    "# Turn \"insulin\" into dummy encoding (No, Steady, Up, Down)\n",
    "df = pd.get_dummies(df, columns=['insulin'])\n",
    "\n",
    "# In \"admission_type_id\" column, merge/simplify values into fewer categories (from 1-8 to 1,3,4,5) and then turn it\n",
    "# into dummy encoding (1: Emergency, 3: Elective, 4: Newborn, 5: Other)\n",
    "df['admission_type_id'] = df['admission_type_id'].replace(2,1)\n",
    "df['admission_type_id'] = df['admission_type_id'].replace(7,1)\n",
    "df['admission_type_id'] = df['admission_type_id'].replace(6,5)\n",
    "df['admission_type_id'] = df['admission_type_id'].replace(8,5)\n",
    "df['admission_type_id'] = df['admission_type_id'].replace({'1': \"Emergency\", '3': \"Elective\", '4': \"Newborn\", '5': \"Other\"})\n",
    "df = pd.get_dummies(df, columns=['admission_type_id'])\n",
    "\n",
    "# In \"admission_source_id\" column, merge/simplify values into fewer categories and then turn it\n",
    "# into dummy encoding (1: Physician Referral, 4: Transfer from a hospital, 9: Transfer from a Skilled Nursing Facility, 11: Other)\n",
    "df['admission_source_id'] = df['admission_source_id'].replace(2,1)\n",
    "df['admission_source_id'] = df['admission_source_id'].replace(3,1)\n",
    "df['admission_source_id'] = df['admission_source_id'].replace(5,4)\n",
    "df['admission_source_id'] = df['admission_source_id'].replace(6,4)\n",
    "df['admission_source_id'] = df['admission_source_id'].replace(10,4)\n",
    "df['admission_source_id'] = df['admission_source_id'].replace(22,4)\n",
    "df['admission_source_id'] = df['admission_source_id'].replace(25,4)\n",
    "df['admission_source_id'] = df['admission_source_id'].replace(15,9)\n",
    "df['admission_source_id'] = df['admission_source_id'].replace(17,9)\n",
    "df['admission_source_id'] = df['admission_source_id'].replace(20,9)\n",
    "df['admission_source_id'] = df['admission_source_id'].replace(21,9)\n",
    "df['admission_source_id'] = df['admission_source_id'].replace(8,11)\n",
    "df['admission_source_id'] = df['admission_source_id'].replace(13,11)\n",
    "df['admission_source_id'] = df['admission_source_id'].replace(14,11)\n",
    "df['admission_source_id'] = df['admission_source_id'].replace({'1': \"Physician_Referral\", '4': \"Transfer_Hospital\", '7': \"Emergency_Room\", '9': \"Transfer_Nursing\", '11': \"Other\"})\n",
    "df = pd.get_dummies(df, columns=['admission_source_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "readmitted\n",
       "NO     50640\n",
       ">30    33246\n",
       "<30    10605\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ------ CHECKING FINAL DATA SET PROPORTION ------\n",
    "df['readmitted'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "readmitted\n",
       "NO     35000\n",
       ">30    33246\n",
       "<30    10605\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ------ RE BALANCE DATA SET  ------\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Put values for 'readmitted' == 1 on a separate dataframe\n",
    "df_1 = df[df.readmitted=='<30']\n",
    "\n",
    "# Delete the rows with 'readmitted' == 1 from the original dataframe\n",
    "df = df[df.readmitted!='<30']\n",
    "\n",
    "# Encode the 'readmitted' column to 1\n",
    "df['readmitted'] = df['readmitted'].replace('>30', 1)\n",
    "df['readmitted'] = df['readmitted'].replace('NO', 0)\n",
    "\n",
    "# Separate majority and minority classes\n",
    "df_majority = df[df.readmitted==0]\n",
    "df_minority = df[df.readmitted==1]\n",
    "\n",
    "# Upsample minority classes\n",
    "df_majority_downsample = resample(df_majority, \n",
    "                                 replace=True,     # sample with replacement\n",
    "                                 n_samples=35000,    # to match majority class\n",
    "                                 random_state=123) # reproducible results\n",
    "\n",
    "\n",
    "# Stack results to original dataframe\n",
    "df = pd.concat([df_majority_downsample, df_minority])\n",
    "\n",
    "# Change values of 1 back to '>30'\n",
    "df['readmitted'] = df['readmitted'].replace(1,'>30')\n",
    "\n",
    "# Stack back rows that were deleted\n",
    "df = pd.concat([df, df_1])\n",
    "\n",
    "# Change values of 0 back to 'NO'\n",
    "df['readmitted'] = df['readmitted'].replace(0, 'NO')\n",
    "\n",
    "# Display new class counts\n",
    "df.readmitted.value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Splitting & normalizing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ SPLIT PREPARED DATASET INTO: X and y  ------\n",
    "X = df.drop('readmitted', axis=1)\n",
    "y = df['readmitted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ NORMALIZE PREPARED DATASET ------\n",
    "# Normalize the dataset using MinMaxScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ SPLIT DATASET INTO TRAIN AND TEST ------\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Training & evaluating multi-classification models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression: 0.531955\n",
      "LDA: 0.527137\n",
      "Random Forest: 0.629090\n",
      "Decision Tree: 0.545270\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Flags' object has no attribute 'c_contiguous'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 33\u001b[0m\n\u001b[1;32m     29\u001b[0m names \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, model \u001b[38;5;129;01min\u001b[39;00m models:\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m# kfold = StratifiedKFold(n_splits=2, random_state=1, shuffle=True)\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m# cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring='accuracy')\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m     cv_results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(cv_results)\n\u001b[1;32m     35\u001b[0m     names\u001b[38;5;241m.\u001b[39mappend(name)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/base.py:705\u001b[0m, in \u001b[0;36mClassifierMixin.score\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;124;03mReturn the mean accuracy on the given test data and labels.\u001b[39;00m\n\u001b[1;32m    682\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[38;5;124;03m    Mean accuracy of ``self.predict(X)`` w.r.t. `y`.\u001b[39;00m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score\n\u001b[0;32m--> 705\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m accuracy_score(y, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m, sample_weight\u001b[38;5;241m=\u001b[39msample_weight)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/neighbors/_classification.py:246\u001b[0m, in \u001b[0;36mKNeighborsClassifier.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    244\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_fit_method\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muniform\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 246\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbrute\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mArgKminClassMode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_usable_for\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetric\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    249\u001b[0m         probabilities \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict_proba(X)\n\u001b[1;32m    250\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputs_2d_:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_pairwise_distances_reduction/_dispatcher.py:471\u001b[0m, in \u001b[0;36mArgKminClassMode.is_usable_for\u001b[0;34m(cls, X, Y, metric)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_usable_for\u001b[39m(\u001b[38;5;28mcls\u001b[39m, X, Y, metric) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m    450\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return True if the dispatcher can be used for the given parameters.\u001b[39;00m\n\u001b[1;32m    451\u001b[0m \n\u001b[1;32m    452\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;124;03m    True if the PairwiseDistancesReduction can be used, else False.\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 471\u001b[0m         \u001b[43mArgKmin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_usable_for\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    472\u001b[0m         \u001b[38;5;66;03m# TODO: Support CSR matrices.\u001b[39;00m\n\u001b[1;32m    473\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m issparse(X)\n\u001b[1;32m    474\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m issparse(Y)\n\u001b[1;32m    475\u001b[0m         \u001b[38;5;66;03m# TODO: implement Euclidean specialization with GEMM.\u001b[39;00m\n\u001b[1;32m    476\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m metric \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meuclidean\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msqeuclidean\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    477\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_pairwise_distances_reduction/_dispatcher.py:115\u001b[0m, in \u001b[0;36mBaseDistancesReductionDispatcher.is_usable_for\u001b[0;34m(cls, X, Y, metric)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_valid_sparse_matrix\u001b[39m(X):\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    103\u001b[0m         isspmatrix_csr(X)\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    110\u001b[0m         X\u001b[38;5;241m.\u001b[39mindices\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m X\u001b[38;5;241m.\u001b[39mindptr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mint32\n\u001b[1;32m    111\u001b[0m     )\n\u001b[1;32m    113\u001b[0m is_usable \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    114\u001b[0m     get_config()\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menable_cython_pairwise_dist\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[43mis_numpy_c_ordered\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mor\u001b[39;00m is_valid_sparse_matrix(X))\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (is_numpy_c_ordered(Y) \u001b[38;5;129;01mor\u001b[39;00m is_valid_sparse_matrix(Y))\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m X\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m Y\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m X\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01min\u001b[39;00m (np\u001b[38;5;241m.\u001b[39mfloat32, np\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m metric \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mvalid_metrics()\n\u001b[1;32m    120\u001b[0m )\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m is_usable\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_pairwise_distances_reduction/_dispatcher.py:99\u001b[0m, in \u001b[0;36mBaseDistancesReductionDispatcher.is_usable_for.<locals>.is_numpy_c_ordered\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_numpy_c_ordered\u001b[39m(X):\n\u001b[0;32m---> 99\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(X, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflags\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflags\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_contiguous\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Flags' object has no attribute 'c_contiguous'"
     ]
    }
   ],
   "source": [
    "# Import multi-classification models\n",
    "from sklearn.linear_model import LogisticRegression # Use OVR strategy\n",
    "from sklearn.svm import SVC # Use OVR strategy\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n",
    "# Define the models\n",
    "models = []\n",
    "models.append(('Logistic Regression', LogisticRegression(multi_class='ovr', solver='liblinear')))\n",
    "#models.append(('SVM', OneVsRestClassifier(SVC())))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('Random Forest', RandomForestClassifier()))\n",
    "#models.append(('XGBoost', XGBClassifier()))\n",
    "models.append(('Decision Tree', DecisionTreeClassifier()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('Naive Bayes', GaussianNB()))\n",
    "\n",
    "\n",
    "# Train and evaluate each model\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "    # kfold = StratifiedKFold(n_splits=2, random_state=1, shuffle=True)\n",
    "    # cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring='accuracy')\n",
    "    cv_results = model.fit(X_train, y_train).score(X_test, y_test)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    print('%s: %f' % (name, cv_results)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
